{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feed-forward nets for image classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/giorgianav/workshop-code/blob/master/Feed_forward_nets_for_image_classification.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "tmaHAc_KHG_5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feed-forward nets for image classification"
      ]
    },
    {
      "metadata": {
        "id": "zgNIWhzoHOIz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. The MNIST dataset"
      ]
    },
    {
      "metadata": {
        "id": "nSxkLXegVDTg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7b2bcd9d-279a-42b1-8bb4-6df101ce085b"
      },
      "cell_type": "code",
      "source": [
        "!pip install mnist"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mnist in /usr/local/lib/python3.6/dist-packages (0.2.1)\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mnist) (1.14.5)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dQiqJyO7E7Ek",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "\n",
        "import mnist\n",
        "train_imgs = mnist.train_images()\n",
        "train_labels = mnist.train_labels()\n",
        "test_imgs = mnist.test_images()\n",
        "test_labels  = mnist.test_labels()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_N0SMBWGUWr2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data standardization\n",
        "\n",
        "Rescale input values to have zero mean and standard deviation of one."
      ]
    },
    {
      "metadata": {
        "id": "DnVAjT-0UWr2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mean, std  = train_imgs.mean(), train_imgs.std()\n",
        "train_imgs = (train_imgs - mean) / std\n",
        "test_imgs = (test_imgs - mean) / std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wTYFRWmsUWr6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### See some inputs"
      ]
    },
    {
      "metadata": {
        "id": "qeftJ_CpE7Eu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CB07PiQ2VdpD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "a98abb9b-0dcf-4620-d979-8dce001ae132"
      },
      "cell_type": "code",
      "source": [
        "idxs = np.random.randint(0, len(train_imgs), 15)\n",
        "imgs = np.concatenate(tuple(train_imgs[idx,:,:] for idx in idxs), axis=1)\n",
        "plt.imshow(imgs)\n",
        "print(\"Labels:\", train_labels[idxs])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labels: [8 6 3 3 1 4 9 8 0 9 9 2 3 2 6]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAA5CAYAAAD5nKpKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f4af426f438>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ZgERUA07IuSr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Building feed forward-networks"
      ]
    },
    {
      "metadata": {
        "id": "j6gHq9UWUWsC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Layers\n",
        "\n",
        "Each layer will have three methods:\n",
        " - `forward` computes and returns ${\\bf y}^{(l)} = f_l\\left({\\bf x}^{(l)}, {\\bf w}^{(l)}\\right)$\n",
        " - `backward` gets $\\frac{\\partial {\\cal L}}{\\partial {\\bf y}^{(l)}}$, and stores $\\frac{\\partial {\\cal L}}{\\partial {\\bf w}^{(l)}}$ internally, and returns $\\frac{\\partial {\\cal L}}{\\partial {\\bf w}^{(l)}}$\n",
        " - `update` modifies parameters ${\\bf w}^{(l)}$ using stored $\\frac{\\partial{\\cal L}}{\\partial{\\bf w}}$"
      ]
    },
    {
      "metadata": {
        "id": "aWMM_flhUWsE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def backward(self, x: np.ndarray, dy: np.ndarray) -> np.ndarray:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def update(self, *args, **kwargs):\n",
        "        pass  # If a layer has no parameters, then this function does nothing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hs7FojYVUWsH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The feed-forward netowork"
      ]
    },
    {
      "metadata": {
        "id": "SNIDMAXdUWsI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FeedForwardNetwork:\n",
        "    \n",
        "    def __init__(self, layers: List[Layer]):\n",
        "        self.layers = layers\n",
        "        \n",
        "    def forward(self, x: np.ndarray, train: bool = True) -> np.ndarray:\n",
        "        self._inputs = []\n",
        "        for layer in self.layers:\n",
        "            if train:\n",
        "                self._inputs.append(x)\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "    \n",
        "    def backward(self, dy:np.ndarray) -> np.ndarray:\n",
        "        # TODO <0> : Compute the backward phase\n",
        "        for layer, x in zip(self.layers[::-1], self._inputs[::-1]):\n",
        "            dy = layer.backward(x, dy)\n",
        "        del self._inputs\n",
        "    \n",
        "    def update(self, *args, **kwargs):\n",
        "        for layer in self.layers:\n",
        "            layer.update(*args, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "phxdPY4fUWsK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The linear layer"
      ]
    },
    {
      "metadata": {
        "id": "S47ZsyKdE7FF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Linear(Layer):\n",
        "    \n",
        "    def __init__(self, insize: int, outsize: int) -> None:\n",
        "        bound = np.sqrt(6. / insize)\n",
        "        self.weight = np.random.uniform(-bound, bound, (insize, outsize))\n",
        "        self.bias = np.zeros((outsize,))\n",
        "        \n",
        "        self.dweight = np.zeros_like(self.weight)\n",
        "        self.dbias = np.zeros_like(self.bias)\n",
        "\n",
        "        \n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        # TODO <1> : compute the output of a linear layer\n",
        "        return x @ self.weight + self.bias\n",
        "        \n",
        "    \n",
        "    def backward(self, x: np.ndarray, dy: np.ndarray) -> np.ndarray:\n",
        "        # TODO <2> : compute dweight, dbias and  return dx\n",
        "        self.dweight = x.T @ dy\n",
        "        self.dbias = dy.sum(axis=0)\n",
        "        return dy @ self.weight.T\n",
        "    \n",
        "    def update(self, mode='SGD', lr=0.001, mu=.9):\n",
        "        if mode == 'SGD':\n",
        "            self.weight -= lr * self.dweight\n",
        "            self.bias -= lr * self.dbias\n",
        "        elif mode == 'Nesterov':\n",
        "            # TODO <9> : compute the nesterov update\n",
        "            raise NotImplementedError\n",
        "        elif mode == 'Adam':\n",
        "            raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EBfmy7wIUWsN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Rectified Linear Unit\n",
        "$$y = \\max\\left(x, 0\\right)$$"
      ]
    },
    {
      "metadata": {
        "id": "QOR1DJiwE7FJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReLU(Layer):\n",
        "    \n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        # TODO <3> : Compute the output of a rectified linear unit\n",
        "        return x * ( x > 0)\n",
        "        # raise NotImplementedError\n",
        "    \n",
        "    def backward(self, x: np.ndarray, dy: np.ndarray) -> np.ndarray:\n",
        "        # TODO <4> : Compute the gradient w.r.t. x\n",
        "        return dy * ( x > 0)\n",
        "        # raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4NrWBTmbI9gW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. The loss function\n",
        "\n",
        "   The negative log likelihood combines a softmax activation, and a cross-entropy cost."
      ]
    },
    {
      "metadata": {
        "id": "YDXiDEu8E7FW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NegativeLogLikelihood:\n",
        "    \n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, y: np.ndarray, t: np.ndarray) -> float:\n",
        "        # TODO <5> : Compute the negative log likelihood\n",
        "        return np.mean(y[np.arange(len(y)),t] - np.log(np.exp(y).sum(axis=1)))\n",
        "        #raise NotImplementedError\n",
        "    \n",
        "    def backward(self, y: np.ndarray, t: np.ndarray) -> np.ndarray:\n",
        "        # TODO <6> : Compute dl/dy\n",
        "        toh = np.zeros_like(y)\n",
        "        toh[np.arange(len(t)), t] = 1\n",
        "        return (y - toh) / y.shape[0]\n",
        "        #raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uz9qM5eHJLNw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Accuracy"
      ]
    },
    {
      "metadata": {
        "id": "3nYfVCBSE7Fe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def accuracy(y: np.ndarray, t: np.ndarray) -> float:\n",
        "    # TODO <7> : Compute accuracy\n",
        "    return np.mean(t == y.argmax(axis=1)) * 100\n",
        "    # raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mIhtzd2gJQF2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Training a neural network"
      ]
    },
    {
      "metadata": {
        "id": "HTbmZv3YE7Fs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 892
        },
        "outputId": "575a3242-22e2-4ed8-f04d-34872fc100c3"
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "HIDDEN_UNITS = 200\n",
        "EPOCHS_NO = 50\n",
        "\n",
        "optimize_args = {'mode': 'SGD', 'lr': .001}\n",
        "\n",
        "net = FeedForwardNetwork([Linear(784, HIDDEN_UNITS),\n",
        "                          ReLU(),\n",
        "                          Linear(HIDDEN_UNITS, 10)])\n",
        "\n",
        "nll = NegativeLogLikelihood()\n",
        "\n",
        "for epoch in range(EPOCHS_NO):\n",
        "    for b_no, idx in enumerate(range(0, len(train_imgs), BATCH_SIZE)):\n",
        "        # 1. Prepare next batch\n",
        "        x = train_imgs[idx:idx + BATCH_SIZE,:,:].reshape(-1, 784)\n",
        "        t = train_labels[idx:idx + BATCH_SIZE]\n",
        "        \n",
        "        # 2. Compute gradient\n",
        "        \n",
        "        # TODO <8> : Compute gradient\n",
        "        #raise NotImplementedError\n",
        "        y = net.forward(x)\n",
        "        loss = nll.forward(y, t)\n",
        "        dy = nll.backward(y, t)\n",
        "        net.backward(dy)\n",
        "        \n",
        "        # 3. Update network parameters\n",
        "        net.update(**optimize_args)\n",
        "        \n",
        "        print(f'\\rEpoch {epoch + 1:02d} '\n",
        "              f'| Batch {b_no:03d} '\n",
        "              f'| Train NLL: {loss:3.5f} '\n",
        "              f'| Train Accuracy: {accuracy(y, t):3.2f} ', end='')\n",
        "\n",
        "    y = net.forward(test_imgs.reshape(-1, 784), train=False)\n",
        "    test_nll = nll.forward(y, test_labels)\n",
        "    print(f' | Test NLL: {test_nll:3.5f} '\n",
        "          f' | Test Accuracy: {accuracy(y, test_labels):3.2f}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 01 | Batch 468 | Train NLL: -2.01516 | Train Accuracy: 35.42  | Test NLL: -1.90825  | Test Accuracy: 47.95\n",
            "Epoch 02 | Batch 468 | Train NLL: -1.94187 | Train Accuracy: 45.83  | Test NLL: -1.85831  | Test Accuracy: 58.82\n",
            "Epoch 03 | Batch 468 | Train NLL: -1.90807 | Train Accuracy: 52.08  | Test NLL: -1.83698  | Test Accuracy: 65.42\n",
            "Epoch 04 | Batch 468 | Train NLL: -1.88622 | Train Accuracy: 60.42  | Test NLL: -1.82514  | Test Accuracy: 69.93\n",
            "Epoch 05 | Batch 468 | Train NLL: -1.87142 | Train Accuracy: 62.50  | Test NLL: -1.81736  | Test Accuracy: 73.35\n",
            "Epoch 06 | Batch 468 | Train NLL: -1.85905 | Train Accuracy: 65.62  | Test NLL: -1.81119  | Test Accuracy: 76.12\n",
            "Epoch 07 | Batch 468 | Train NLL: -1.84763 | Train Accuracy: 69.79  | Test NLL: -1.80568  | Test Accuracy: 77.85\n",
            "Epoch 08 | Batch 468 | Train NLL: -1.83804 | Train Accuracy: 73.96  | Test NLL: -1.80025  | Test Accuracy: 79.49\n",
            "Epoch 09 | Batch 468 | Train NLL: -1.82986 | Train Accuracy: 77.08  | Test NLL: -1.79485  | Test Accuracy: 81.02\n",
            "Epoch 10 | Batch 468 | Train NLL: -1.82293 | Train Accuracy: 78.12  | Test NLL: -1.78958  | Test Accuracy: 82.29\n",
            "Epoch 11 | Batch 468 | Train NLL: -1.81631 | Train Accuracy: 80.21  | Test NLL: -1.78430  | Test Accuracy: 83.00\n",
            "Epoch 12 | Batch 468 | Train NLL: -1.80996 | Train Accuracy: 81.25  | Test NLL: -1.77916  | Test Accuracy: 83.87\n",
            "Epoch 13 | Batch 468 | Train NLL: -1.80414 | Train Accuracy: 82.29  | Test NLL: -1.77417  | Test Accuracy: 84.65\n",
            "Epoch 14 | Batch 468 | Train NLL: -1.79858 | Train Accuracy: 82.29  | Test NLL: -1.76927  | Test Accuracy: 85.25\n",
            "Epoch 15 | Batch 468 | Train NLL: -1.79316 | Train Accuracy: 84.38  | Test NLL: -1.76455  | Test Accuracy: 85.87\n",
            "Epoch 16 | Batch 468 | Train NLL: -1.78794 | Train Accuracy: 85.42  | Test NLL: -1.76002  | Test Accuracy: 86.41\n",
            "Epoch 17 | Batch 468 | Train NLL: -1.78293 | Train Accuracy: 85.42  | Test NLL: -1.75566  | Test Accuracy: 86.81\n",
            "Epoch 18 | Batch 468 | Train NLL: -1.77827 | Train Accuracy: 85.42  | Test NLL: -1.75148  | Test Accuracy: 87.16\n",
            "Epoch 19 | Batch 468 | Train NLL: -1.77367 | Train Accuracy: 85.42  | Test NLL: -1.74747  | Test Accuracy: 87.48\n",
            "Epoch 20 | Batch 468 | Train NLL: -1.76933 | Train Accuracy: 85.42  | Test NLL: -1.74364  | Test Accuracy: 87.74\n",
            "Epoch 21 | Batch 468 | Train NLL: -1.76508 | Train Accuracy: 87.50  | Test NLL: -1.73992  | Test Accuracy: 88.11\n",
            "Epoch 22 | Batch 468 | Train NLL: -1.76105 | Train Accuracy: 87.50  | Test NLL: -1.73636  | Test Accuracy: 88.36\n",
            "Epoch 23 | Batch 468 | Train NLL: -1.75722 | Train Accuracy: 87.50  | Test NLL: -1.73293  | Test Accuracy: 88.67\n",
            "Epoch 24 | Batch 468 | Train NLL: -1.75357 | Train Accuracy: 87.50  | Test NLL: -1.72962  | Test Accuracy: 88.94\n",
            "Epoch 25 | Batch 468 | Train NLL: -1.74995 | Train Accuracy: 87.50  | Test NLL: -1.72639  | Test Accuracy: 89.09\n",
            "Epoch 26 | Batch 468 | Train NLL: -1.74649 | Train Accuracy: 89.58  | Test NLL: -1.72331  | Test Accuracy: 89.29\n",
            "Epoch 27 | Batch 468 | Train NLL: -1.74327 | Train Accuracy: 89.58  | Test NLL: -1.72039  | Test Accuracy: 89.45\n",
            "Epoch 28 | Batch 468 | Train NLL: -1.74013 | Train Accuracy: 89.58  | Test NLL: -1.71755  | Test Accuracy: 89.62\n",
            "Epoch 29 | Batch 468 | Train NLL: -1.73716 | Train Accuracy: 89.58  | Test NLL: -1.71478  | Test Accuracy: 89.77\n",
            "Epoch 30 | Batch 468 | Train NLL: -1.73436 | Train Accuracy: 89.58  | Test NLL: -1.71216  | Test Accuracy: 89.89\n",
            "Epoch 31 | Batch 468 | Train NLL: -1.73168 | Train Accuracy: 89.58  | Test NLL: -1.70968  | Test Accuracy: 90.04\n",
            "Epoch 32 | Batch 468 | Train NLL: -1.72908 | Train Accuracy: 89.58  | Test NLL: -1.70728  | Test Accuracy: 90.09\n",
            "Epoch 33 | Batch 468 | Train NLL: -1.72655 | Train Accuracy: 89.58  | Test NLL: -1.70495  | Test Accuracy: 90.17\n",
            "Epoch 34 | Batch 468 | Train NLL: -1.72412 | Train Accuracy: 89.58  | Test NLL: -1.70273  | Test Accuracy: 90.27\n",
            "Epoch 35 | Batch 468 | Train NLL: -1.72177 | Train Accuracy: 89.58  | Test NLL: -1.70054  | Test Accuracy: 90.40\n",
            "Epoch 36 | Batch 468 | Train NLL: -1.71944 | Train Accuracy: 89.58  | Test NLL: -1.69840  | Test Accuracy: 90.46\n",
            "Epoch 37 | Batch 468 | Train NLL: -1.71722 | Train Accuracy: 89.58  | Test NLL: -1.69643  | Test Accuracy: 90.57\n",
            "Epoch 38 | Batch 468 | Train NLL: -1.71507 | Train Accuracy: 89.58  | Test NLL: -1.69449  | Test Accuracy: 90.65\n",
            "Epoch 39 | Batch 468 | Train NLL: -1.71299 | Train Accuracy: 90.62  | Test NLL: -1.69264  | Test Accuracy: 90.73\n",
            "Epoch 40 | Batch 468 | Train NLL: -1.71088 | Train Accuracy: 90.62  | Test NLL: -1.69082  | Test Accuracy: 90.77\n",
            "Epoch 41 | Batch 468 | Train NLL: -1.70880 | Train Accuracy: 90.62  | Test NLL: -1.68908  | Test Accuracy: 90.83\n",
            "Epoch 42 | Batch 468 | Train NLL: -1.70676 | Train Accuracy: 90.62  | Test NLL: -1.68740  | Test Accuracy: 90.86\n",
            "Epoch 43 | Batch 468 | Train NLL: -1.70480 | Train Accuracy: 90.62  | Test NLL: -1.68575  | Test Accuracy: 90.90\n",
            "Epoch 44 | Batch 468 | Train NLL: -1.70289 | Train Accuracy: 90.62  | Test NLL: -1.68413  | Test Accuracy: 90.98\n",
            "Epoch 45 | Batch 468 | Train NLL: -1.70105 | Train Accuracy: 90.62  | Test NLL: -1.68255  | Test Accuracy: 91.07\n",
            "Epoch 46 | Batch 468 | Train NLL: -1.69924 | Train Accuracy: 90.62  | Test NLL: -1.68101  | Test Accuracy: 91.11\n",
            "Epoch 47 | Batch 468 | Train NLL: -1.69742 | Train Accuracy: 90.62  | Test NLL: -1.67950  | Test Accuracy: 91.15\n",
            "Epoch 48 | Batch 468 | Train NLL: -1.69564 | Train Accuracy: 90.62  | Test NLL: -1.67806  | Test Accuracy: 91.20\n",
            "Epoch 49 | Batch 468 | Train NLL: -1.69395 | Train Accuracy: 90.62  | Test NLL: -1.67669  | Test Accuracy: 91.27\n",
            "Epoch 50 | Batch 468 | Train NLL: -1.69231 | Train Accuracy: 90.62  | Test NLL: -1.67533  | Test Accuracy: 91.26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EgMbww7XUWsb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}