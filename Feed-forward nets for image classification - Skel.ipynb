{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feed-forward nets for image classification.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "tmaHAc_KHG_5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feed-forward nets for image classification"
      ]
    },
    {
      "metadata": {
        "id": "zgNIWhzoHOIz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. The MNIST dataset"
      ]
    },
    {
      "metadata": {
        "id": "dQiqJyO7E7Ek",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install mnist\n",
        "\n",
        "import mnist\n",
        "train_imgs = mnist.train_images()\n",
        "train_labels = mnist.train_labels()\n",
        "test_imgs = mnist.test_images()\n",
        "test_labels  = mnist.test_labels()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "21g9pOfgtJIS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data standardization\n",
        "\n",
        "Rescale input values to have zero mean and standard deviation of one."
      ]
    },
    {
      "metadata": {
        "id": "8QhTRG5KtJIT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mean, std  = train_imgs.mean(), train_imgs.std()\n",
        "train_imgs = (train_imgs - mean) / std\n",
        "test_imgs = (test_imgs - mean) / std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d_KNznZItJIV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### See some inputs"
      ]
    },
    {
      "metadata": {
        "id": "qeftJ_CpE7Eu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZgERUA07IuSr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Building feed forward-networks"
      ]
    },
    {
      "metadata": {
        "id": "_enPBByctJIf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Layers\n",
        "\n",
        "Each layer will have three methods:\n",
        " - `forward` computes and returns ${\\bf y}^{(l)} = f_l\\left({\\bf x}^{(l)}, {\\bf w}^{(l)}\\right)$\n",
        " - `backward` gets $\\frac{\\partial {\\cal L}}{\\partial {\\bf y}^{(l)}}$, and stores $\\frac{\\partial {\\cal L}}{\\partial {\\bf w}^{(l)}}$ internally, and returns $\\frac{\\partial {\\cal L}}{\\partial {\\bf w}^{(l)}}$\n",
        " - `update` modifies parameters ${\\bf w}^{(l)}$ using stored $\\frac{\\partial{\\cal L}}{\\partial{\\bf w}}$"
      ]
    },
    {
      "metadata": {
        "id": "my-pSQujtJIg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def backward(self, x: np.ndarray, dy: np.ndarray) -> np.ndarray:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    def update(self, *args, **kwargs):\n",
        "        pass  # If a layer has no parameters, then this function does nothing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GCxe4VfFtJIk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The feed-forward netowork"
      ]
    },
    {
      "metadata": {
        "id": "DkMnyth8tJIl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FeedForwardNetwork:\n",
        "    \n",
        "    def __init__(self, layers: List[Layer]):\n",
        "        self.layers = layers\n",
        "        \n",
        "    def forward(self, x: np.ndarray, train: bool = True) -> np.ndarray:\n",
        "        self._inputs = []\n",
        "        for layer in self.layers:\n",
        "            if train:\n",
        "                self._inputs.append(x)\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "    \n",
        "    def backward(self, dy:np.ndarray) -> np.ndarray:\n",
        "        # TODO <0> : Compute the backward phase\n",
        "        raise NotImplementedError\n",
        "        del self._inputs\n",
        "    \n",
        "    def update(self, *args, **kwargs):\n",
        "        for layer in self.layers:\n",
        "            layer.update(*args, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H6q_H__ntJIr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The linear layer"
      ]
    },
    {
      "metadata": {
        "id": "S47ZsyKdE7FF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Linear(Layer):\n",
        "    \n",
        "    def __init__(self, insize: int, outsize: int) -> None:\n",
        "        bound = np.sqrt(6. / insize)\n",
        "        self.weight = np.random.uniform(-bound, bound, (insize, outsize))\n",
        "        self.bias = np.zeros((outsize,))\n",
        "        \n",
        "        self.dweight = np.zeros_like(self.weight)\n",
        "        self.dbias = np.zeros_like(self.bias)\n",
        "\n",
        "        \n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        # TODO <1> : compute the output of a linear layer\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def backward(self, x: np.ndarray, dy: np.ndarray) -> np.ndarray:\n",
        "        # TODO <2> : compute dweight, dbias and  return dx\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def update(self, mode='SGD', lr=0.001, mu=.9):\n",
        "        if mode == 'SGD':\n",
        "            self.weight -= lr * self.dweight\n",
        "            self.bias -= lr * self.dbias\n",
        "        elif mode == 'Nesterov':\n",
        "            # TODO <9> : compute the nesterov update\n",
        "            raise NotImplementedError\n",
        "        elif mode == 'Adam':\n",
        "            raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XrX6Md8ttJIv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The Rectified Linear Unit\n",
        "$$y = \\max\\left(x, 0\\right)$$"
      ]
    },
    {
      "metadata": {
        "id": "QOR1DJiwE7FJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReLU(Layer):\n",
        "    \n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        # TODO <3> : Compute the output of a rectified linear unit\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def backward(self, x: np.ndarray, dy: np.ndarray) -> np.ndarray:\n",
        "        # TODO <4> : Compute the gradient w.r.t. x\n",
        "        raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4NrWBTmbI9gW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. The loss function\n",
        "\n",
        "   The negative log likelihood combines a softmax activation, and a cross-entropy cost."
      ]
    },
    {
      "metadata": {
        "id": "YDXiDEu8E7FW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NegativeLogLikelihood:\n",
        "    \n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, y: np.ndarray, t: np.ndarray) -> float:\n",
        "        # TODO <5> : Compute the negative log likelihood\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    def backward(self, y: np.ndarray, t: np.ndarray) -> np.ndarray:\n",
        "        # TODO <6> : Compute dl/dy\n",
        "        raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uz9qM5eHJLNw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Accuracy"
      ]
    },
    {
      "metadata": {
        "id": "3nYfVCBSE7Fe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def accuracy(y: np.ndarray, t: np.ndarray) -> float:\n",
        "    # TODO <7> : Compute accuracy\n",
        "    raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mIhtzd2gJQF2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Training a neural network"
      ]
    },
    {
      "metadata": {
        "id": "HTbmZv3YE7Fs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "HIDDEN_UNITS = 200\n",
        "EPOCHS_NO = 50\n",
        "\n",
        "optimize_args = {'mode': 'SGD', 'lr': .001}\n",
        "\n",
        "net = FeedForwardNetwork([Linear(784, HIDDEN_UNITS),\n",
        "                          ReLU(),\n",
        "                          Linear(HIDDEN_UNITS, 10)])\n",
        "nll = NegativeLogLikelihood()\n",
        "\n",
        "for epoch in range(EPOCHS_NO):\n",
        "    for b_no, idx in enumerate(range(0, len(train_imgs), BATCH_SIZE)):\n",
        "        # 1. Prepare next batch\n",
        "        x = train_imgs[idx:idx + BATCH_SIZE,:,:].reshape(-1, 784)\n",
        "        t = train_labels[idx:idx + BATCH_SIZE]\n",
        "        \n",
        "        # 2. Compute gradient\n",
        "        \n",
        "        # TODO <8> : Compute gradient\n",
        "        raise NotImplementedError\n",
        "        \n",
        "        # 3. Update network parameters\n",
        "        net.update(**optimize_args)\n",
        "        \n",
        "        print(f'\\rEpoch {epoch + 1:02d} '\n",
        "              f'| Batch {b_no:03d} '\n",
        "              f'| Train NLL: {loss:3.5f} '\n",
        "              f'| Train Accuracy: {accuracy(y, t):3.2f} ', end='')\n",
        "\n",
        "    y = net.forward(test_imgs.reshape(-1, 784), train=False)\n",
        "    test_nll = nll.forward(y, test_labels)\n",
        "    print(f' | Test NLL: {test_nll:3.5f} '\n",
        "          f' | Test Accuracy: {accuracy(y, test_labels):3.2f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "csevd6rttJJE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}